{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProblemSet5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTNt+T3JrsoHGhbXRXHND+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SonOf1998/ProblemSet5/blob/main/ProblemSet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAk6Oe8b3mVp"
      },
      "source": [
        "As hyperas needs the notebook's relative path as a parameter, we first need to download it directly from the github repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtDt63ThtGnC"
      },
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "working_dir = os.getcwd()\n",
        "local_zip = os.path.join(working_dir, 'main.zip')\n",
        "\n",
        "url = 'https://github.com/SonOf1998/ProblemSet5/archive/main.zip'\n",
        "urlretrieve(url,local_zip)\n",
        "\n",
        "# Extract zip file\n",
        "zip_ref = zipfile.ZipFile(local_zip,'r') \n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n",
        "# Discard zip file\n",
        "if os.path.exists(local_zip):\n",
        "  os.remove(local_zip)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShR74_0J4BM4"
      },
      "source": [
        "The dataset needs to be created in a function to support hyperas.  \n",
        "\n",
        "Pictures needs to be min-max scaled first. Also, we'd like our labels one hot encoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivnTPCPR2xW7"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def dataset():\n",
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "  y_train = to_categorical(y_train, 10)\n",
        "  y_test =  to_categorical(y_test, 10)\n",
        "\n",
        "  # min-max normalization\n",
        "  x_train = x_train / 255\n",
        "  x_test  = x_test / 255\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl9jLSvnDB-Z",
        "outputId": "af27c79d-e467-4b39-9bdb-e77ae59a1dd1"
      },
      "source": [
        "!pip install hyperas\n",
        "!pip install hyperopt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.0.8)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.4.3)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.0.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.5.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.3.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.18.5)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.11.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.41.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.2.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.7.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.11.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.6.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (20.0.0)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (5.3.5)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (1.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hyperas) (4.4.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (20.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (2.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (50.3.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert->hyperas) (2.4.7)\n",
            "Installing collected packages: hyperas\n",
            "Successfully installed hyperas-0.4.1\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.18.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.5)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.11.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JriuqsT8jvy"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, ReLU, LeakyReLU\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import hyperas\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGo6i9zG4c_4"
      },
      "source": [
        "Model creation is also something that hyperas forces us to do in a function.  \n",
        "\n",
        "The model structure is more or less copied from here: https://gist.github.com/JulieProst/8000610500a67fda4b76e07efe585552#file-keras_model-py\n",
        "\n",
        "I used the exact same method for hyperparameter optimalization and logging with which we were experimenting during the course and which could be found here: \n",
        "https://github.com/BME-SmartLab-Education/vitmav45/blob/master/12/hyperas_fashionmnist_pub.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV1gFCfSDqoQ"
      },
      "source": [
        "def create_model(x_train, x_test, y_train, y_test):\n",
        "\n",
        "  act = {{choice(['relu', 'leakyrelu'])}}\n",
        "  opti = {{choice(['rmsprop', 'adam', 'sgd'])}}\n",
        "  kernel_size = {{choice([3, 4])}}\n",
        "  dropout_1 = {{uniform(0, 0.5)}}\n",
        "  dropout_2 = {{uniform(0, 0.5)}}\n",
        "  dropout_3 = {{uniform(0, 0.5)}}\n",
        "  unit_size = {{choice([64, 128, 256])}}\n",
        "  batch_size = {{choice([64, 128, 256])}}\n",
        "  flt_layer = {{choice(['flatten', 'globalavgpooling'])}}\n",
        "\n",
        "  activation = None\n",
        "  if act == 'relu':\n",
        "    activation=ReLU()\n",
        "  elif act == 'leakyrelu':\n",
        "    activation=LeakyReLU()\n",
        "\n",
        "  flattening_layer = None\n",
        "  if flt_layer == 'flatten':\n",
        "    flattening_layer = Flatten()\n",
        "  elif flt_layer == 'globalavgpooling':\n",
        "    flattening_layer = GlobalAveragePooling2D()\n",
        "\n",
        "  early_stop = EarlyStopping(monitor='val_accuracy', patience=3, verbose=0)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, kernel_size, input_shape=(32, 32, 3)))\n",
        "  model.add(activation)\n",
        "  model.add(Conv2D(16, kernel_size))\n",
        "  model.add(activation)\n",
        "  model.add(MaxPooling2D(2))\n",
        "  model.add(Dropout(dropout_1))\n",
        "  model.add(Conv2D(32, kernel_size))\n",
        "  model.add(activation)\n",
        "  model.add(Conv2D(64, kernel_size))\n",
        "  model.add(activation)\n",
        "  model.add(MaxPooling2D(2))\n",
        "  model.add(Dropout(dropout_2))\n",
        "  model.add(flattening_layer)\n",
        "  model.add(Dense(unit_size))\n",
        "  model.add(activation)\n",
        "  model.add(Dropout(dropout_3))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=opti, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  results = model.fit(x_train, y_train, batch_size=batch_size, epochs=100, verbose=0,\n",
        "                      validation_data=(x_test, y_test), callbacks=[early_stop], shuffle=True)\n",
        "\n",
        "  # model needs to be freed to avoid RAM issues in Colab\n",
        "  backend.clear_session()\n",
        "\n",
        "  best_val_acc = np.amax(results.history['val_accuracy']) \n",
        "\n",
        "  with open('hyperas-log.csv', 'a') as csv_file:\n",
        "      csv_file.write(str(kernel_size) + ';')\n",
        "      csv_file.write(str(dropout_1) + ';')\n",
        "      csv_file.write(str(dropout_2) + ';')\n",
        "      csv_file.write(str(dropout_3) + ';')\n",
        "      csv_file.write(str(act) + ';')\n",
        "      csv_file.write(str(unit_size) + ';')\n",
        "      csv_file.write(str(opti) + ';')\n",
        "      csv_file.write(str(flt_layer) + ';')\n",
        "      csv_file.write(str(batch_size) + ';')\n",
        "      csv_file.write(str(best_val_acc) + '\\n')\n",
        "\n",
        "\n",
        "  return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EXAYNfB-RVS"
      },
      "source": [
        "with open('hyperas-log.csv', 'w') as csv_file:\n",
        "  csv_file.write(\"kernel_size\" + ';')\n",
        "  csv_file.write(\"dropout_1\" + ';')\n",
        "  csv_file.write(\"dropout_2\" + ';')\n",
        "  csv_file.write(\"dropout_3\" + ';')\n",
        "  csv_file.write(\"activation\" + ';')\n",
        "  csv_file.write(\"unit_size\" + ';')\n",
        "  csv_file.write(\"opti\" + ';')\n",
        "  csv_file.write(\"flattening_layer\" + ';')\n",
        "  csv_file.write(\"batch_size\" + ';')\n",
        "  csv_file.write(\"best_val_acc\" + '\\n')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDOBEDpu7pHh"
      },
      "source": [
        "We have both the model creating and the dataset creating functions, so we can start the optimiaztion process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5ol1W_f0L1Oy",
        "outputId": "aa62c9fc-04b3-43ba-e77f-d068f3566000"
      },
      "source": [
        "best_run, best_model = optim.minimize(model=create_model, \n",
        "                                      data=dataset, algo=tpe.suggest, \n",
        "                                      max_evals=100, trials=Trials(), \n",
        "                                      notebook_name=\"ProblemSet5-main/ProblemSet5\", \n",
        "                                      verbose=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    import os\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from urllib.request import urlretrieve\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import requests\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import zipfile\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.datasets import cifar10\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.utils import to_categorical\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D, ReLU, LeakyReLU\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.callbacks import EarlyStopping\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras import backend\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import hyperas\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'act': hp.choice('act', ['relu', 'leakyrelu']),\n",
            "        'opti': hp.choice('opti', ['rmsprop', 'adam', 'sgd']),\n",
            "        'kernel_size': hp.choice('kernel_size', [3, 4]),\n",
            "        'dropout_1': hp.uniform('dropout_1', 0, 0.5),\n",
            "        'dropout_1_1': hp.uniform('dropout_1_1', 0, 0.5),\n",
            "        'dropout_1_2': hp.uniform('dropout_1_2', 0, 0.5),\n",
            "        'unit_size': hp.choice('unit_size', [64, 128, 256]),\n",
            "        'unit_size_1': hp.choice('unit_size_1', [64, 128, 256]),\n",
            "        'flt_layer': hp.choice('flt_layer', ['flatten', 'globalavgpooling']),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "  1: \n",
            "  2: (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
            "  3: \n",
            "  4: y_train = to_categorical(y_train, 10)\n",
            "  5: y_test =  to_categorical(y_test, 10)\n",
            "  6: \n",
            "  7: # min-max normalization\n",
            "  8: x_train = x_train / 255\n",
            "  9: x_test  = x_test / 255\n",
            " 10: \n",
            " 11: \n",
            " 12: \n",
            " 13: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3: \n",
            "   4:   act = space['act']\n",
            "   5:   opti = space['opti']\n",
            "   6:   kernel_size = space['kernel_size']\n",
            "   7:   dropout_1 = space['dropout_1']\n",
            "   8:   dropout_2 = space['dropout_1_1']\n",
            "   9:   dropout_3 = space['dropout_1_2']\n",
            "  10:   unit_size = space['unit_size']\n",
            "  11:   batch_size = space['unit_size_1']\n",
            "  12:   flt_layer = space['flt_layer']\n",
            "  13: \n",
            "  14:   activation = None\n",
            "  15:   if act == 'relu':\n",
            "  16:     activation=ReLU()\n",
            "  17:   elif act == 'leakyrelu':\n",
            "  18:     activation=LeakyReLU()\n",
            "  19: \n",
            "  20:   flattening_layer = None\n",
            "  21:   if flt_layer == 'flatten':\n",
            "  22:     flattening_layer = Flatten()\n",
            "  23:   elif flt_layer == 'globalavgpooling':\n",
            "  24:     flattening_layer = GlobalAveragePooling2D()\n",
            "  25: \n",
            "  26:   early_stop = EarlyStopping(monitor='val_accuracy', patience=3, verbose=0)\n",
            "  27: \n",
            "  28:   model = Sequential()\n",
            "  29:   model.add(Conv2D(16, kernel_size, input_shape=(32, 32, 3)))\n",
            "  30:   model.add(activation)\n",
            "  31:   model.add(Conv2D(16, kernel_size))\n",
            "  32:   model.add(activation)\n",
            "  33:   model.add(MaxPooling2D(2))\n",
            "  34:   model.add(Dropout(dropout_1))\n",
            "  35:   model.add(Conv2D(32, kernel_size))\n",
            "  36:   model.add(activation)\n",
            "  37:   model.add(Conv2D(64, kernel_size))\n",
            "  38:   model.add(activation)\n",
            "  39:   model.add(MaxPooling2D(2))\n",
            "  40:   model.add(Dropout(dropout_2))\n",
            "  41:   model.add(flattening_layer)\n",
            "  42:   model.add(Dense(unit_size))\n",
            "  43:   model.add(activation)\n",
            "  44:   model.add(Dropout(dropout_3))\n",
            "  45:   model.add(Dense(10, activation='softmax'))\n",
            "  46: \n",
            "  47:   model.compile(optimizer=opti, loss='categorical_crossentropy', metrics=['accuracy'])\n",
            "  48:   results = model.fit(x_train, y_train, batch_size=batch_size, epochs=100, verbose=0,\n",
            "  49:                       validation_data=(x_test, y_test), callbacks=[early_stop], shuffle=True)\n",
            "  50: \n",
            "  51:   # model needs to be freed to avoid RAM issues in Colab\n",
            "  52:   backend.clear_session()\n",
            "  53: \n",
            "  54:   best_val_acc = np.amax(results.history['val_accuracy']) \n",
            "  55: \n",
            "  56:   with open('hyperas-log.csv', 'a') as csv_file:\n",
            "  57:       csv_file.write(str(kernel_size) + ';')\n",
            "  58:       csv_file.write(str(dropout_1) + ';')\n",
            "  59:       csv_file.write(str(dropout_2) + ';')\n",
            "  60:       csv_file.write(str(dropout_3) + ';')\n",
            "  61:       csv_file.write(str(act) + ';')\n",
            "  62:       csv_file.write(str(unit_size) + ';')\n",
            "  63:       csv_file.write(str(opti) + ';')\n",
            "  64:       csv_file.write(str(flt_layer) + ';')\n",
            "  65:       csv_file.write(str(batch_size) + ';')\n",
            "  66:       csv_file.write(str(best_val_acc) + '\\n')\n",
            "  67: \n",
            "  68: \n",
            "  69:   return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}\n",
            "  70: \n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "  1%|          | 1/100 [01:00<1:40:38, 60.99s/it, best loss: -0.6243000030517578]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-37a17426c57b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                       \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                       \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ProblemSet5-main/ProblemSet5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                       verbose=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    137\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m              return_argmin=True),\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Flatten' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWXdNDmn8D13"
      },
      "source": [
        "Let's print the log in descending order with respect to 'best validation accuracy' and examine the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM5qzIf1My3J"
      },
      "source": [
        "import pandas\n",
        "df = pandas.read_csv('hyperas-log.csv', delimiter=';')\n",
        "df.sort_values(by=['best_val_acc'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt8OpqKq8nAG"
      },
      "source": [
        "#Observations:\n",
        "\n",
        "> the best models mostly used adam as optimizer, and the worst models were using sgd\n",
        "\n",
        "> The number of neurons in the last hidden layer were mostly 64 units in case of the best performing models\n",
        "\n",
        "> "
      ]
    }
  ]
}