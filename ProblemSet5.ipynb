{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProblemSet5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNl9WAJAbuZ/2rutqI+GGA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SonOf1998/ProblemSet5/blob/main/ProblemSet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAk6Oe8b3mVp"
      },
      "source": [
        "As hyperas needs the notebook's relative path as a parameter, we first need to download it directly from the github repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtDt63ThtGnC"
      },
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "working_dir = os.getcwd()\n",
        "local_zip = os.path.join(working_dir, 'main.zip')\n",
        "\n",
        "url = 'https://github.com/SonOf1998/ProblemSet5/archive/main.zip'\n",
        "urlretrieve(url,local_zip)\n",
        "\n",
        "# Extract zip file\n",
        "zip_ref = zipfile.ZipFile(local_zip,'r') \n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n",
        "# Discard zip file\n",
        "if os.path.exists(local_zip):\n",
        "  os.remove(local_zip)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShR74_0J4BM4"
      },
      "source": [
        "The dataset needs to be created in a function to support hyperas.  \n",
        "\n",
        "Pictures needs to be min-max scaled first. Also, we'd like our labels one hot encoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivnTPCPR2xW7"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def dataset():\n",
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "  y_train = to_categorical(y_train, 10)\n",
        "  y_test =  to_categorical(y_test, 10)\n",
        "\n",
        "  # min-max normalization\n",
        "  x_train = x_train / 255\n",
        "  x_test  = x_test / 255\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl9jLSvnDB-Z",
        "outputId": "82a2f257-c6e1-43cd-f5ee-b1a782031896"
      },
      "source": [
        "!pip install hyperas\n",
        "!pip install hyperopt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.4.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.0.8)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.11.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.7.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.6.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.10.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.15.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.11.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.3.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.5.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->hyperas) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (20.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.1.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (1.5.0)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.3.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (20.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert->hyperas) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.0->notebook->jupyter->hyperas) (2.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.2.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (50.3.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.7.5)\n",
            "Installing collected packages: hyperas\n",
            "Successfully installed hyperas-0.4.1\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.11.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.18.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JriuqsT8jvy"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, ReLU, LeakyReLU\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import hyperas\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGo6i9zG4c_4"
      },
      "source": [
        "Model creation is also something that hyperas forces us to do in a function.  \n",
        "\n",
        "The model structure is more or less copied from here: https://gist.github.com/JulieProst/8000610500a67fda4b76e07efe585552#file-keras_model-py\n",
        "\n",
        "I used the exact same method for hyperparameter optimalization and logging with which we were experimenting during the course and which could be found here: \n",
        "https://github.com/BME-SmartLab-Education/vitmav45/blob/master/12/hyperas_fashionmnist_pub.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV1gFCfSDqoQ"
      },
      "source": [
        "def create_model(x_train, x_test, y_train, y_test):\n",
        "\n",
        "  act = {{choice(['relu', 'leakyrelu'])}}\n",
        "  opti = {{choice(['rmsprop', 'adam', 'sgd'])}}\n",
        "  kernel_size = {{choice([3, 4])}}\n",
        "  dropout_1 = {{uniform(0, 0.5)}}\n",
        "  dropout_2 = {{uniform(0, 0.5)}}\n",
        "  dropout_3 = {{uniform(0, 0.5)}}\n",
        "  unit_size = {{choice([64, 128, 256])}}\n",
        "  batch_size = {{choice([64, 128, 256])}}\n",
        "  flt_layer = {{choice(['flatten', 'globalavgpooling'])}}\n",
        "\n",
        "  activation = None\n",
        "  if act == 'relu':\n",
        "    activation=ReLU()\n",
        "  elif act == 'leakyrelu':\n",
        "    activation=LeakyReLU()\n",
        "\n",
        "  flattening_layer = None\n",
        "  if flt_layer == 'flatten':\n",
        "    flattening_layer = Flatten()\n",
        "  elif flt_layer == 'globalavgpooling':\n",
        "    flattening_layer = GlobalAveragePooling2D()\n",
        "\n",
        "  early_stop = EarlyStopping(monitor='val_accuracy', patience=3, verbose=0)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, kernel_size, input_shape=(32, 32, 3)))\n",
        "  model.add(activation)\n",
        "  model.add(Conv2D(16, kernel_size))\n",
        "  model.add(activation)\n",
        "  model.add(MaxPooling2D(2))\n",
        "  model.add(Dropout(dropout_1))\n",
        "  model.add(Conv2D(32, kernel_size))\n",
        "  model.add(activation)\n",
        "  model.add(Conv2D(64, kernel_size))\n",
        "  model.add(activation)\n",
        "  model.add(MaxPooling2D(2))\n",
        "  model.add(Dropout(dropout_2))\n",
        "  model.add(flattening_layer)\n",
        "  model.add(Dense(unit_size))\n",
        "  model.add(activation)\n",
        "  model.add(Dropout(dropout_3))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=opti, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  results = model.fit(x_train, y_train, batch_size=batch_size, epochs=100, verbose=0,\n",
        "                      validation_data=(x_test, y_test), callbacks=[early_stop], shuffle=True)\n",
        "\n",
        "  # model needs to be freed to avoid RAM issues in Colab\n",
        "  backend.clear_session()\n",
        "\n",
        "  best_val_acc = np.amax(results.history['val_accuracy']) \n",
        "\n",
        "  with open('hyperas-log.csv', 'a') as csv_file:\n",
        "      csv_file.write(str(kernel_size) + ';')\n",
        "      csv_file.write(str(dropout_1) + ';')\n",
        "      csv_file.write(str(dropout_2) + ';')\n",
        "      csv_file.write(str(dropout_3) + ';')\n",
        "      csv_file.write(str(act) + ';')\n",
        "      csv_file.write(str(unit_size) + ';')\n",
        "      csv_file.write(str(opti) + ';')\n",
        "      csv_file.write(str(flt_layer) + ';')\n",
        "      csv_file.write(str(batch_size) + ';')\n",
        "      csv_file.write(str(best_val_acc) + '\\n')\n",
        "\n",
        "\n",
        "  return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EXAYNfB-RVS"
      },
      "source": [
        "with open('hyperas-log.csv', 'w') as csv_file:\n",
        "  csv_file.write(\"kernel_size\" + ';')\n",
        "  csv_file.write(\"dropout_1\" + ';')\n",
        "  csv_file.write(\"dropout_2\" + ';')\n",
        "  csv_file.write(\"dropout_3\" + ';')\n",
        "  csv_file.write(\"activation\" + ';')\n",
        "  csv_file.write(\"unit_size\" + ';')\n",
        "  csv_file.write(\"opti\" + ';')\n",
        "  csv_file.write(\"flattening_layer\" + ';')\n",
        "  csv_file.write(\"batch_size\" + ';')\n",
        "  csv_file.write(\"best_val_acc\" + '\\n')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDOBEDpu7pHh"
      },
      "source": [
        "We have both the model creating and the dataset creating functions, so we can start the optimization process.\n",
        "\n",
        "(Google Colab kept on crashing due to timeouts and RAM shortages so I couldn't manage to get 100 models tried out) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ol1W_f0L1Oy",
        "outputId": "ed14b31f-5ba7-4863-9d49-75ff25f48cc8"
      },
      "source": [
        "best_run, best_model = optim.minimize(model=create_model, \n",
        "                                      data=dataset, algo=tpe.suggest, \n",
        "                                      max_evals=100, trials=Trials(), \n",
        "                                      notebook_name=\"ProblemSet5-main/ProblemSet5\", \n",
        "                                      verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    import os\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from urllib.request import urlretrieve\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import requests\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import zipfile\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.datasets import cifar10\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.utils import to_categorical\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, ReLU, LeakyReLU\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.callbacks import EarlyStopping\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras import backend\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import hyperas\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'act': hp.choice('act', ['relu', 'leakyrelu']),\n",
            "        'opti': hp.choice('opti', ['rmsprop', 'adam', 'sgd']),\n",
            "        'kernel_size': hp.choice('kernel_size', [3, 4]),\n",
            "        'dropout_1': hp.uniform('dropout_1', 0, 0.5),\n",
            "        'dropout_1_1': hp.uniform('dropout_1_1', 0, 0.5),\n",
            "        'dropout_1_2': hp.uniform('dropout_1_2', 0, 0.5),\n",
            "        'unit_size': hp.choice('unit_size', [64, 128, 256]),\n",
            "        'unit_size_1': hp.choice('unit_size_1', [64, 128, 256]),\n",
            "        'flt_layer': hp.choice('flt_layer', ['flatten', 'globalavgpooling']),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "  1: \n",
            "  2: (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
            "  3: \n",
            "  4: y_train = to_categorical(y_train, 10)\n",
            "  5: y_test =  to_categorical(y_test, 10)\n",
            "  6: \n",
            "  7: # min-max normalization\n",
            "  8: x_train = x_train / 255\n",
            "  9: x_test  = x_test / 255\n",
            " 10: \n",
            " 11: \n",
            " 12: \n",
            " 13: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3: \n",
            "   4:   act = space['act']\n",
            "   5:   opti = space['opti']\n",
            "   6:   kernel_size = space['kernel_size']\n",
            "   7:   dropout_1 = space['dropout_1']\n",
            "   8:   dropout_2 = space['dropout_1_1']\n",
            "   9:   dropout_3 = space['dropout_1_2']\n",
            "  10:   unit_size = space['unit_size']\n",
            "  11:   batch_size = space['unit_size_1']\n",
            "  12:   flt_layer = space['flt_layer']\n",
            "  13: \n",
            "  14:   activation = None\n",
            "  15:   if act == 'relu':\n",
            "  16:     activation=ReLU()\n",
            "  17:   elif act == 'leakyrelu':\n",
            "  18:     activation=LeakyReLU()\n",
            "  19: \n",
            "  20:   flattening_layer = None\n",
            "  21:   if flt_layer == 'flatten':\n",
            "  22:     flattening_layer = Flatten()\n",
            "  23:   elif flt_layer == 'globalavgpooling':\n",
            "  24:     flattening_layer = GlobalAveragePooling2D()\n",
            "  25: \n",
            "  26:   early_stop = EarlyStopping(monitor='val_accuracy', patience=3, verbose=0)\n",
            "  27: \n",
            "  28:   model = Sequential()\n",
            "  29:   model.add(Conv2D(16, kernel_size, input_shape=(32, 32, 3)))\n",
            "  30:   model.add(activation)\n",
            "  31:   model.add(Conv2D(16, kernel_size))\n",
            "  32:   model.add(activation)\n",
            "  33:   model.add(MaxPooling2D(2))\n",
            "  34:   model.add(Dropout(dropout_1))\n",
            "  35:   model.add(Conv2D(32, kernel_size))\n",
            "  36:   model.add(activation)\n",
            "  37:   model.add(Conv2D(64, kernel_size))\n",
            "  38:   model.add(activation)\n",
            "  39:   model.add(MaxPooling2D(2))\n",
            "  40:   model.add(Dropout(dropout_2))\n",
            "  41:   model.add(flattening_layer)\n",
            "  42:   model.add(Dense(unit_size))\n",
            "  43:   model.add(activation)\n",
            "  44:   model.add(Dropout(dropout_3))\n",
            "  45:   model.add(Dense(10, activation='softmax'))\n",
            "  46: \n",
            "  47:   model.compile(optimizer=opti, loss='categorical_crossentropy', metrics=['accuracy'])\n",
            "  48:   results = model.fit(x_train, y_train, batch_size=batch_size, epochs=100, verbose=0,\n",
            "  49:                       validation_data=(x_test, y_test), callbacks=[early_stop], shuffle=True)\n",
            "  50: \n",
            "  51:   # model needs to be freed to avoid RAM issues in Colab\n",
            "  52:   backend.clear_session()\n",
            "  53: \n",
            "  54:   best_val_acc = np.amax(results.history['val_accuracy']) \n",
            "  55: \n",
            "  56:   with open('hyperas-log.csv', 'a') as csv_file:\n",
            "  57:       csv_file.write(str(kernel_size) + ';')\n",
            "  58:       csv_file.write(str(dropout_1) + ';')\n",
            "  59:       csv_file.write(str(dropout_2) + ';')\n",
            "  60:       csv_file.write(str(dropout_3) + ';')\n",
            "  61:       csv_file.write(str(act) + ';')\n",
            "  62:       csv_file.write(str(unit_size) + ';')\n",
            "  63:       csv_file.write(str(opti) + ';')\n",
            "  64:       csv_file.write(str(flt_layer) + ';')\n",
            "  65:       csv_file.write(str(batch_size) + ';')\n",
            "  66:       csv_file.write(str(best_val_acc) + '\\n')\n",
            "  67: \n",
            "  68: \n",
            "  69:   return {'loss': -best_val_acc, 'status': STATUS_OK, 'model': model}\n",
            "  70: \n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            " 46%|████▌     | 46/100 [48:02<56:36, 62.90s/it, best loss: -0.7795000076293945]  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWXdNDmn8D13"
      },
      "source": [
        "Let's print the log in descending order with respect to 'best validation accuracy' and examine the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gM5qzIf1My3J",
        "outputId": "40630fdd-2269-4c37-dacc-8a4233dde46d"
      },
      "source": [
        "import pandas\n",
        "df = pandas.read_csv('hyperas-log.csv', delimiter=';')\n",
        "df.sort_values(by=['best_val_acc'], ascending=False)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>kernel_size</th>\n",
              "      <th>dropout_1</th>\n",
              "      <th>dropout_2</th>\n",
              "      <th>dropout_3</th>\n",
              "      <th>activation</th>\n",
              "      <th>unit_size</th>\n",
              "      <th>opti</th>\n",
              "      <th>flattening_layer</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>best_val_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.217021</td>\n",
              "      <td>0.348754</td>\n",
              "      <td>0.257640</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>3</td>\n",
              "      <td>0.267871</td>\n",
              "      <td>0.477925</td>\n",
              "      <td>0.261250</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>3</td>\n",
              "      <td>0.234329</td>\n",
              "      <td>0.475882</td>\n",
              "      <td>0.063723</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>3</td>\n",
              "      <td>0.230268</td>\n",
              "      <td>0.470779</td>\n",
              "      <td>0.021042</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3</td>\n",
              "      <td>0.016111</td>\n",
              "      <td>0.449891</td>\n",
              "      <td>0.076055</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3</td>\n",
              "      <td>0.316514</td>\n",
              "      <td>0.326275</td>\n",
              "      <td>0.443791</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>3</td>\n",
              "      <td>0.259608</td>\n",
              "      <td>0.351246</td>\n",
              "      <td>0.222275</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3</td>\n",
              "      <td>0.173965</td>\n",
              "      <td>0.310184</td>\n",
              "      <td>0.310113</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>128</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>0.105498</td>\n",
              "      <td>0.146636</td>\n",
              "      <td>0.365115</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>128</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>3</td>\n",
              "      <td>0.175273</td>\n",
              "      <td>0.192582</td>\n",
              "      <td>0.084893</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>3</td>\n",
              "      <td>0.028941</td>\n",
              "      <td>0.435036</td>\n",
              "      <td>0.092508</td>\n",
              "      <td>relu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3</td>\n",
              "      <td>0.174108</td>\n",
              "      <td>0.321425</td>\n",
              "      <td>0.316377</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>128</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>3</td>\n",
              "      <td>0.422709</td>\n",
              "      <td>0.460950</td>\n",
              "      <td>0.277462</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>3</td>\n",
              "      <td>0.247041</td>\n",
              "      <td>0.199557</td>\n",
              "      <td>0.407884</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>64</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>3</td>\n",
              "      <td>0.306239</td>\n",
              "      <td>0.250605</td>\n",
              "      <td>0.428350</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3</td>\n",
              "      <td>0.279855</td>\n",
              "      <td>0.319409</td>\n",
              "      <td>0.308873</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>128</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>0.085130</td>\n",
              "      <td>0.382897</td>\n",
              "      <td>0.372315</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>64</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>3</td>\n",
              "      <td>0.386734</td>\n",
              "      <td>0.361977</td>\n",
              "      <td>0.173909</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>4</td>\n",
              "      <td>0.204367</td>\n",
              "      <td>0.370281</td>\n",
              "      <td>0.478843</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3</td>\n",
              "      <td>0.317912</td>\n",
              "      <td>0.442876</td>\n",
              "      <td>0.497503</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3</td>\n",
              "      <td>0.338589</td>\n",
              "      <td>0.353773</td>\n",
              "      <td>0.439356</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.7360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>3</td>\n",
              "      <td>0.271345</td>\n",
              "      <td>0.107743</td>\n",
              "      <td>0.397420</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>128</td>\n",
              "      <td>adam</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>4</td>\n",
              "      <td>0.200202</td>\n",
              "      <td>0.495474</td>\n",
              "      <td>0.333742</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4</td>\n",
              "      <td>0.204683</td>\n",
              "      <td>0.386524</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>adam</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4</td>\n",
              "      <td>0.395730</td>\n",
              "      <td>0.247744</td>\n",
              "      <td>0.233589</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>64</td>\n",
              "      <td>adam</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>3</td>\n",
              "      <td>0.369605</td>\n",
              "      <td>0.272806</td>\n",
              "      <td>0.271314</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>4</td>\n",
              "      <td>0.165355</td>\n",
              "      <td>0.346375</td>\n",
              "      <td>0.155951</td>\n",
              "      <td>relu</td>\n",
              "      <td>64</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>flatten</td>\n",
              "      <td>128</td>\n",
              "      <td>0.7072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "      <td>0.224146</td>\n",
              "      <td>0.401010</td>\n",
              "      <td>0.202404</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>64</td>\n",
              "      <td>adam</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>64</td>\n",
              "      <td>0.7042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>3</td>\n",
              "      <td>0.341975</td>\n",
              "      <td>0.496027</td>\n",
              "      <td>0.242370</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>64</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.6927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4</td>\n",
              "      <td>0.074444</td>\n",
              "      <td>0.071629</td>\n",
              "      <td>0.126271</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>flatten</td>\n",
              "      <td>128</td>\n",
              "      <td>0.6893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4</td>\n",
              "      <td>0.052331</td>\n",
              "      <td>0.003015</td>\n",
              "      <td>0.154532</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>64</td>\n",
              "      <td>0.6757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3</td>\n",
              "      <td>0.123367</td>\n",
              "      <td>0.135355</td>\n",
              "      <td>0.049615</td>\n",
              "      <td>relu</td>\n",
              "      <td>64</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>128</td>\n",
              "      <td>0.6731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.498777</td>\n",
              "      <td>0.495601</td>\n",
              "      <td>0.382308</td>\n",
              "      <td>relu</td>\n",
              "      <td>128</td>\n",
              "      <td>adam</td>\n",
              "      <td>flatten</td>\n",
              "      <td>128</td>\n",
              "      <td>0.6689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4</td>\n",
              "      <td>0.407988</td>\n",
              "      <td>0.393533</td>\n",
              "      <td>0.262316</td>\n",
              "      <td>relu</td>\n",
              "      <td>256</td>\n",
              "      <td>sgd</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>64</td>\n",
              "      <td>0.6384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3</td>\n",
              "      <td>0.479308</td>\n",
              "      <td>0.411328</td>\n",
              "      <td>0.008747</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>128</td>\n",
              "      <td>0.6288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.268686</td>\n",
              "      <td>0.427364</td>\n",
              "      <td>0.448256</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>sgd</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>64</td>\n",
              "      <td>0.6074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>4</td>\n",
              "      <td>0.292592</td>\n",
              "      <td>0.421427</td>\n",
              "      <td>0.049120</td>\n",
              "      <td>relu</td>\n",
              "      <td>256</td>\n",
              "      <td>sgd</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>3</td>\n",
              "      <td>0.121286</td>\n",
              "      <td>0.236514</td>\n",
              "      <td>0.352199</td>\n",
              "      <td>relu</td>\n",
              "      <td>256</td>\n",
              "      <td>sgd</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.5709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>3</td>\n",
              "      <td>0.141553</td>\n",
              "      <td>0.289731</td>\n",
              "      <td>0.211238</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>sgd</td>\n",
              "      <td>flatten</td>\n",
              "      <td>256</td>\n",
              "      <td>0.5395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4</td>\n",
              "      <td>0.444100</td>\n",
              "      <td>0.193789</td>\n",
              "      <td>0.180586</td>\n",
              "      <td>relu</td>\n",
              "      <td>64</td>\n",
              "      <td>sgd</td>\n",
              "      <td>flatten</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>0.142754</td>\n",
              "      <td>0.268561</td>\n",
              "      <td>0.371161</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>sgd</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>128</td>\n",
              "      <td>0.5233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>0.092293</td>\n",
              "      <td>0.496949</td>\n",
              "      <td>0.114191</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>64</td>\n",
              "      <td>sgd</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>0.056230</td>\n",
              "      <td>0.160376</td>\n",
              "      <td>0.367107</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>128</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>256</td>\n",
              "      <td>0.4922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3</td>\n",
              "      <td>0.344072</td>\n",
              "      <td>0.129872</td>\n",
              "      <td>0.298841</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>sgd</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>128</td>\n",
              "      <td>0.4820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.485930</td>\n",
              "      <td>0.072572</td>\n",
              "      <td>0.236343</td>\n",
              "      <td>leakyrelu</td>\n",
              "      <td>256</td>\n",
              "      <td>sgd</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>128</td>\n",
              "      <td>0.4636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>0.095205</td>\n",
              "      <td>0.040319</td>\n",
              "      <td>0.488641</td>\n",
              "      <td>relu</td>\n",
              "      <td>256</td>\n",
              "      <td>sgd</td>\n",
              "      <td>globalavgpooling</td>\n",
              "      <td>256</td>\n",
              "      <td>0.3957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    kernel_size  dropout_1  ...  batch_size  best_val_acc\n",
              "1             3   0.217021  ...          64        0.7795\n",
              "31            3   0.267871  ...          64        0.7791\n",
              "27            3   0.234329  ...         256        0.7791\n",
              "29            3   0.230268  ...          64        0.7786\n",
              "28            3   0.016111  ...         256        0.7785\n",
              "23            3   0.316514  ...         256        0.7765\n",
              "32            3   0.259608  ...          64        0.7744\n",
              "20            3   0.173965  ...         256        0.7710\n",
              "7             3   0.105498  ...         256        0.7677\n",
              "30            3   0.175273  ...         256        0.7644\n",
              "36            3   0.028941  ...         256        0.7610\n",
              "21            3   0.174108  ...         256        0.7598\n",
              "40            3   0.422709  ...         256        0.7592\n",
              "38            3   0.247041  ...         256        0.7589\n",
              "26            3   0.306239  ...          64        0.7557\n",
              "22            3   0.279855  ...         256        0.7517\n",
              "9             3   0.085130  ...         128        0.7472\n",
              "33            3   0.386734  ...         256        0.7412\n",
              "43            4   0.204367  ...          64        0.7363\n",
              "24            3   0.317912  ...          64        0.7360\n",
              "25            3   0.338589  ...         256        0.7360\n",
              "44            3   0.271345  ...          64        0.7352\n",
              "35            4   0.200202  ...          64        0.7270\n",
              "16            4   0.204683  ...         128        0.7215\n",
              "14            4   0.395730  ...         128        0.7197\n",
              "34            3   0.369605  ...          64        0.7171\n",
              "45            4   0.165355  ...         128        0.7072\n",
              "17            3   0.224146  ...          64        0.7042\n",
              "41            3   0.341975  ...          64        0.6927\n",
              "15            4   0.074444  ...         128        0.6893\n",
              "13            4   0.052331  ...          64        0.6757\n",
              "11            3   0.123367  ...         128        0.6731\n",
              "3             4   0.498777  ...         128        0.6689\n",
              "8             4   0.407988  ...          64        0.6384\n",
              "19            3   0.479308  ...         128        0.6288\n",
              "4             4   0.268686  ...          64        0.6074\n",
              "39            4   0.292592  ...          64        0.5798\n",
              "42            3   0.121286  ...         256        0.5709\n",
              "37            3   0.141553  ...         256        0.5395\n",
              "18            4   0.444100  ...          64        0.5331\n",
              "12            3   0.142754  ...         128        0.5233\n",
              "6             3   0.092293  ...          64        0.5027\n",
              "0             3   0.056230  ...         256        0.4922\n",
              "10            3   0.344072  ...         128        0.4820\n",
              "2             3   0.485930  ...         128        0.4636\n",
              "5             3   0.095205  ...         256        0.3957\n",
              "\n",
              "[46 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt8OpqKq8nAG"
      },
      "source": [
        "#Observations:\n",
        "\n",
        "> the best models mostly used adam as optimizer, and the worst models were using sgd\n",
        "\n",
        "> The number of neurons in the last hidden layer were mostly 256 units in case of the best performing models\n",
        "\n",
        "> LeakyReLu overall performs better than regular ReLu\n",
        "\n",
        "> Prefer Flatten over GlobalAveragePooling2D\n",
        "\n",
        "> We cannot really say that much about the optimal batch size\n",
        "\n",
        "> Regarding the dropoutout rates we can't really say that much either. Relatively high dropout rate on the 2nd dropout layer worked good for the top few models in the list."
      ]
    }
  ]
}